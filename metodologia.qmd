# Metodologia {#sec-metodologia}

As principais fórmulas adotadas têm sua fundamentação especialmente determinada em @kutner2004.



Para o cumprimento dos objetivos de pesquisa, será usado o arcabouço teórico estatístico relacionado aos modelos de regressão linear. Em síntese, os modelos de regressão linear são modelos que buscam quantificar e qualificar as relações entre uma variável depentente --- a ser explicada --- e uma ou mais variáveis independentes, que auxiliam na explicação da variável dependente. 

Como se trata de uma relação de dependência no sentido estatístico, não há necessariamente uma relação de causalidade entre as variáveis. Ainda assim, a relação de dependência pode ser usada para a previsão de valores da variável dependente, a partir de valores conhecidos das variáveis independentes.

A estrutura geral de um modelo de regressão linear é dada pela equação:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_n X_{ni} + \varepsilon_i 
\end{equation}

em que $y$ é a variável dependente, $x_1, x_2, \ldots, x_n$ são as variáveis independentes, $\beta_0, \beta_1, \ldots, \beta_n$ são os parâmetros do modelo e $\varepsilon_i$ é o erro aleatório.

## Seleção de variáveis

O processo de seleção de variáveis envolve processos que ajudam a identificar as variáveis relevantes para o modelo. Antes, é preciso conhecer os tipos de variáveis que podem estar presentes no modelo para além dos formatos tradicional das variáveis como são coletadas.

Alguns critérios auxiliam na seleção das variáveis do model de regressão linear a ser utilizado,  como a análise do $R^{2}$, $R^{2}$ ajustado, Critério de Pressão de Mallows (Cp) e Critério de Informação Bayesiano (BIC). Nessa seleção, busca-se uma boa relaçao entre capacidade explicativa/preditiva e parcimoniosidade do modelo.

O coeficiente de determinação ($R^{2}$) é uma medida de ajuste do modelo, que indica a proporção da variância da variável dependente que é explicada pelas variáveis independentes. Ele é calculado por: $$R^{2} = \frac{SQ_{reg}}{SQ_{tot}}$$ em que $SQ_{reg}$ é a soma dos quadrados da regressão e $SQ_{tot}$ é a soma dos quadrados totais. 

O $R^{2}$ ajustado é uma medida de ajuste do modelo que parte do coeficiente de determinação, mas penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente. Sua fórmula é dada por $$R_{ajustado}^{2}  = 1 - \frac{(n - 1)}{n - p}\frac{SQ_{erros}}{SQ_{tot}}$$ em que $SQ_{erros}$ é a soma dos quadrados dos erros, $n$ é o número de observações e $p$ é o número de variáveis independentes.

O Critério de Pressão de Mallows (Cp) é uma medida de ajuste do modelo que penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente. É calculado por $$Cp = \frac{SQ_{erros}}{MSE (X_1, \ldots, X_{p-1})} - (n - 2p)$$ em que $SQ_{erros}$ é a soma dos quadrados dos erros, $MSE$ é o erro médio quadrático, $n$ é o número de observações e $p$ é o número de parâmetros. 

Nesse caso, quando não há viés na regressão do modelo de base para comparação, o valor esperado de $C_p$ é aproximadamente $p$  [@kutner2004 p. 358].

O Critério de Informação Bayesiano (BIC) é uma medida de ajuste do modelo que penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente.




### Modelos de mais de uma ordem

Os modelos de mais de uma ordem são aqueles em que a variável dependente é explicada por uma ou mais variáveis independentes que podem estar em forma de alguma potência inteira maior do que 1. São os chamados "modelos polinomiais" [@kutner2004 p. 294]. Há duas razões principais para isso: 

1. A relação entre a variável explicada e as variáveis explicativas é curvilínea; ou
2. Quando a relação entre as variáveis não é curvilínea, mas pode ser aproximada por uma curva.

Esta última razão tem aplicabilidade comum, e faz parte das hipóteses do presente estudo.

Um exemplo de modelo de mais de uma ordem é o modelo quadrático, dado pela equação:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_{1, 1} X_{1i}^2 + \varepsilon_i
\end{equation} em que $Y_i$ é a variável dependente, $X_{1i}$ é a variável independente, $\beta_0$ é o intercepto, $\beta_1$ é o coeficiente da variável independente e $\beta_{1, 1}$ é o coeficiente da variável independente elevada ao quadrado.

Entretanto, é preciso estar atento às complicações que fórmulas quadráticas ou superiores podem acrescer à interpretação dos resultados. A depender do sinal do coeficiente da variável independente elevada ao quadrado, a curva pode ter concavidade para cima ou para baixo. Em geral, a interpretação mais relevante está em torno de eventual ponto de inflexão (mínimo ou máximo), se este fizer parte do intervalo de observação da variável independente.


### Variáveis categóricas

Variáveis categóricas também podem ser usadas em modelos de regressão linear, desde que sejam transformadas em variáveis binárias. A transformação é feita por meio da criação de novas colunas, que assumem o valor 1 quando a categoria está presente e 0 quando a categoria está ausente.

Para $n$ categorias distintas, são necessárias $n-1$ colunas, pois a última categoria é a referência para as demais e estará representada pelo valor do intercepto do modelo quando as demais categorias assumirem valor $0$. Nesse caso, há uma varição da reta de regressão para cada categoria, indicando uma alteração homogênea sobre o nível da variável resposta sob efeito de todas as demais variáveis.

Um exemplo de variável categórica é a filiação ou não a uma escola de medicina. Considerando $X_{1}"$ como a variável categórica, $X_{2}$ outra variável quantitativa do modelo, a interpretação do modelo se dá da seguinte forma:   

\begin{equation}
\begin{split}
E[Y] = \beta_0 + \beta_1 (1) + \beta_2 X_{2}  = (\beta_0 + \beta_1) + \beta_2 X_{2} \:\:\:\:\:\: & \text{, se } X_{1} = 1 \\
E[Y] = \beta_0 + \beta_1 (0) + \beta_2 X_{2}  = \beta_0 + \beta_2 X_{2} \:\:\:\:\:\: & \text{, se } X_{1} = 0
\end{split}
\end{equation}


Com essa construção, a interpretação do modelo se dá diretamente avaliando a presentaça ou não da variável categórica de interesse, mantendo as demais variáveis constantes.

### Variáveis com interação

Quando um modelo de regressão linear possui variáveis sem interação entre elas, diz-se tratar de um "modelo aditivo" [@kutner2004]. Entretanto, quando isso ocorre, as variáveis devem aparecer sob a forma de produto no modelo, como no exemplo a seguir:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_{1, 2} X_{1i} X_{2i} + \varepsilon_i
\end{equation}

Nesse caso, o efeito de $X_1$ sobre $Y$ depende do valor de $X_2$, e vice-versa. A interpretação do modelo envolve fazer a análise de efeito de cada variável não aditiva a partir de um dado nível da outra variável com que ela se relaciona. Nesse caso, o efeito da variável $X_1$ sobre $Y$ dado $X_2$ constante é dada por: $$\beta_1 + \beta_{1, 2} X_{2i}$$.

Esse prodecidmento deve ser realizado para todas as formas de interação.

### Procedimentos de seleção de variáveis (*forward, backward e stepwise*)

## Pressupostos de um modelo linear

Um modelo de regressão linear apresenta alguns pressupostos, que devem ser verificados para que o modelo seja considerado adequado. São eles:

1. Linearidade: a relação entre as variáveis deve ser linear. Caso contrário, é necessário transformar as variáveis para que a relação se torne linear. Este pressuposto pode ser verificado por meio de gráficos de dispersão entre as variáveis e os resíduos;

2. Normalidade: os erros devem ser normalmente distribuídos, o que se pode verificar ao analisar os resíduos do modelo. Pode ser verificado por meio do teste de Shapiro-Wilk e pela visualização do gráfico de distribuição normal dos resíduos;

3. Homocedasticidade: os erros devem ter variância constante, o que se pode verificar ao analisar os resíduos do modelo em relação às variáveis independentes. Costuma ser verificado por meio do teste de Breusch-Pagan; e

4. Independência: os erros devem ser independentes, o que também se pode verificar ao analisar os resíduos do modelo em relação às variáveis independentes. Pode ser verificado por meio do teste de Durbin-Watson;

5. Ausência de multicolinearidade entre as variáveis: as variáveis independentes não devem ser correlacionadas entre si, o que se pode verificar ao analisar a matriz de correlação entre as variáveis independentes. O caso da multicolinearidade perfeita pode fazer com que o modelo tenha múltiplas soluções, o que torna a estimação sem validade. O caso da multicolinearidade imperfeita pode fazer com que o modelo tenha solução, mas com variâncias muito grandes --- com elevada chance de não rejeição da hipótese nula de parâmetro zero, estimativas com sinais em desacordo com toda a literatura existente, o que torna a estimação também problemática.




## Estimação dos parâmetros


### Testes de ausência de regressão e de significância dos parâmetros

## Validação do modelo 

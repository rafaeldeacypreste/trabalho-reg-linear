# Metodologia {#sec-metodologia}

As principais fórmulas adotadas têm sua fundamentação especialmente determinada em @kutner2004.



Para o cumprimento dos objetivos de pesquisa, será usado o arcabouço teórico estatístico relacionado aos modelos de regressão linear. Em síntese, os modelos de regressão linear são modelos que buscam quantificar e qualificar as relações entre uma variável depentente --- a ser explicada --- e uma ou mais variáveis independentes, que auxiliam na explicação da variável dependente. 

Como se trata de uma relação de dependência no sentido estatístico, não há necessariamente uma relação de causalidade entre as variáveis. Ainda assim, a relação de dependência pode ser usada para a previsão de valores da variável dependente, a partir de valores conhecidos das variáveis independentes.

A estrutura geral de um modelo de regressão linear é dada pela equação:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \ldots + \beta_n X_{ni} + \varepsilon_i 
\end{equation}

em que $y$ é a variável dependente, $x_1, x_2, \ldots, x_n$ são as variáveis independentes, $\beta_0, \beta_1, \ldots, \beta_n$ são os parâmetros do modelo e $\varepsilon_i$ é o erro aleatório.

## Seleção de variáveis

O processo de seleção de variáveis envolve processos que ajudam a identificar as variáveis relevantes para o modelo. Antes, é preciso conhecer os tipos de variáveis que podem estar presentes no modelo para além dos formatos tradicional das variáveis como são coletadas.

Alguns critérios auxiliam na seleção das variáveis do modelo de regressão linear a ser utilizado,  como a análise do $R^{2}$, $R^{2}$ ajustado, Critério de Pressão de Mallows (Cp) e Critério de Informação Bayesiano (BIC). Nessa seleção, busca-se uma boa relaçao entre capacidade explicativa/preditiva e parcimoniosidade do modelo.

O coeficiente de determinação ($R^{2}$) é uma medida de ajuste do modelo, que indica a proporção da variância da variável dependente que é explicada pelas variáveis independentes. Ele é calculado por: $$R^{2} = \frac{SQ_{reg}}{SQ_{tot}}$$ em que $SQ_{reg}$ é a soma dos quadrados da regressão e $SQ_{tot}$ é a soma dos quadrados totais. 

O $R^{2}$ ajustado é uma medida de ajuste do modelo que parte do coeficiente de determinação, mas penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente. Sua fórmula é dada por $$R_{ajustado}^{2}  = 1 - \frac{(n - 1)}{n - p}\frac{SQ_{erros}}{SQ_{tot}}$$ em que $SQ_{erros}$ é a soma dos quadrados dos erros, $n$ é o número de observações e $p$ é o número de variáveis independentes.

O Critério de Pressão de Mallows (Cp) é uma medida de ajuste do modelo que penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente. É calculado por $$Cp = \frac{SQ_{erros}}{MSE (X_1, \ldots, X_{p-1})} - (n - 2p)$$ em que $SQ_{erros}$ é a soma dos quadrados dos erros, $MSE$ é o erro médio quadrático, $n$ é o número de observações e $p$ é o número de parâmetros. 

Nesse caso, quando não há viés na regressão do modelo de base para comparação, o valor esperado de $C_p$ é aproximadamente $p$  [@kutner2004 p. 358].

O Critério de Informação Bayesiano (BIC) é uma medida de ajuste do modelo que penaliza a inclusão de variáveis que não contribuem para a explicação da variável dependente. É calculado por $$BIC = n\ln(SQ_{erros, p}) - n\ln(n) + p\ln(n)$$ 



### Modelos de mais de uma ordem

Os modelos de mais de uma ordem são aqueles em que a variável dependente é explicada por uma ou mais variáveis independentes que podem estar em forma de alguma potência inteira maior do que 1. São os chamados "modelos polinomiais" [@kutner2004 p. 294]. Há duas razões principais para isso: 

1. A relação entre a variável explicada e as variáveis explicativas é curvilínea; ou
2. Quando a relação entre as variáveis não é curvilínea, mas pode ser aproximada por uma curva.

Esta última razão tem aplicabilidade comum, e faz parte das hipóteses do presente estudo.

Um exemplo de modelo de mais de uma ordem é o modelo quadrático, dado pela equação:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_{1, 1} X_{1i}^2 + \varepsilon_i
\end{equation} em que $Y_i$ é a variável dependente, $X_{1i}$ é a variável independente, $\beta_0$ é o intercepto, $\beta_1$ é o coeficiente da variável independente e $\beta_{1, 1}$ é o coeficiente da variável independente elevada ao quadrado.

Entretanto, é preciso estar atento às complicações que fórmulas quadráticas ou superiores podem acrescer à interpretação dos resultados. A depender do sinal do coeficiente da variável independente elevada ao quadrado, a curva pode ter concavidade para cima ou para baixo. Em geral, a interpretação mais relevante está em torno de eventual ponto de inflexão (mínimo ou máximo), se este fizer parte do intervalo de observação da variável independente.


### Variáveis categóricas

Variáveis categóricas também podem ser usadas em modelos de regressão linear, desde que sejam transformadas em variáveis binárias. A transformação é feita por meio da criação de novas colunas, que assumem o valor 1 quando a categoria está presente e 0 quando a categoria está ausente.

Para $n$ categorias distintas, são necessárias $n-1$ colunas, pois a última categoria é a referência para as demais e estará representada pelo valor do intercepto do modelo quando as demais categorias assumirem valor $0$. Nesse caso, há uma varição da reta de regressão para cada categoria, indicando uma alteração homogênea sobre o nível da variável resposta sob efeito de todas as demais variáveis.

Um exemplo de variável categórica é a filiação ou não a uma escola de medicina. Considerando $X_{1}"$ como a variável categórica, $X_{2}$ outra variável quantitativa do modelo, a interpretação do modelo se dá da seguinte forma:   

\begin{equation}
\begin{split}
E[Y] = \beta_0 + \beta_1 (1) + \beta_2 X_{2}  = (\beta_0 + \beta_1) + \beta_2 X_{2} \:\:\:\:\:\: & \text{, se } X_{1} = 1 \\
E[Y] = \beta_0 + \beta_1 (0) + \beta_2 X_{2}  = \beta_0 + \beta_2 X_{2} \:\:\:\:\:\: & \text{, se } X_{1} = 0
\end{split}
\end{equation}


Com essa construção, a interpretação do modelo se dá diretamente avaliando a presença ou não da variável categórica de interesse, mantendo as demais variáveis constantes.

### Variáveis com interação

Quando um modelo de regressão linear possui variáveis sem interação entre elas, diz-se tratar de um "modelo aditivo" [@kutner2004]. Entretanto, quando isso ocorre, as variáveis devem aparecer sob a forma de produto no modelo, como no exemplo a seguir:

\begin{equation}
Y_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \beta_{1, 2} X_{1i} X_{2i} + \varepsilon_i
\end{equation}

Nesse caso, o efeito de $X_1$ sobre $Y$ depende do valor de $X_2$, e vice-versa. A interpretação do modelo envolve fazer a análise de efeito de cada variável não aditiva a partir de um dado nível da outra variável com que ela se relaciona. Nesse caso, o efeito da variável $X_1$ sobre $Y$ dado $X_2$ constante é dada por: $$\beta_1 + \beta_{1, 2} X_{2i}$$.

Esse prodecidmento deve ser realizado para todas as formas de interação.

### Procedimentos de seleção de variáveis (*forward, backward e stepwise*)

Há também procedimentos de seleção de variáveis que podem ser usados para a seleção de variáveis. São eles:

1. *Forward*: o procedimento parte de um modelo com apenas o intercepto e vai adicionando variáveis, uma a uma, até que não seja possível adicionar mais nenhuma variável com significância estatística. A adição de variáveis é feita com base no no menor p-valor.

2. *Backward*: fazendo o processo inverso do anterior, o procedimento parte de um modelo com todas as variáveis e vai retirando variáveis do modelo, uma a uma de acordo com seu p-valor, até que todas as variáveis sejam significativas do ponto de vista estatístico.

3. *Stepwise*: o procedimento parte de um modelo com apenas o intercepto e vai adicionando ou retirando variáveis, uma a uma, até que não seja possível adicionar ou seja necessário retirar alguma variável com significância estatística. O procedimento termina quando se encontra o "melhor" modelo [@kutner2004 pp. 364-6]. 




## Pressupostos de um modelo linear

Um modelo de regressão linear apresenta alguns pressupostos, que devem ser verificados para que o modelo seja considerado adequado. São eles:

1. Linearidade: a relação entre as variáveis deve ser linear. Caso contrário, é necessário transformar as variáveis para que a relação se torne linear. Este pressuposto pode ser verificado por meio de gráficos de dispersão entre as variáveis e os resíduos;

2. Normalidade: os erros devem ser normalmente distribuídos, o que se pode verificar ao analisar os resíduos do modelo. Pode ser verificado por meio do teste de Shapiro-Wilk e pela visualização do gráfico de distribuição normal dos resíduos;

3. Homocedasticidade: os erros devem ter variância constante, o que se pode verificar ao analisar os resíduos do modelo em relação às variáveis independentes. Costuma ser verificado por meio do teste de Breusch-Pagan; e

4. Independência: os erros devem ser independentes, o que também se pode verificar ao analisar os resíduos do modelo em relação às variáveis independentes. Pode ser verificado por meio do teste de Durbin-Watson;

5. Ausência de multicolinearidade entre as variáveis: as variáveis independentes não devem ser correlacionadas entre si, o que se pode verificar ao analisar a matriz de correlação entre as variáveis independentes. O caso da multicolinearidade perfeita pode fazer com que o modelo tenha múltiplas soluções, o que torna a estimação sem validade. O caso da multicolinearidade imperfeita pode fazer com que o modelo tenha solução, mas com variâncias muito grandes --- com elevada chance de não rejeição da hipótese nula de parâmetro zero, estimativas com sinais em desacordo com toda a literatura existente, o que torna a estimação também problemática.




## Estimação dos parâmetros

Os parâmetros do modelo são estimados por meio do método dos mínimos quadrados ordinários (MQO). O método consiste em minimizar a soma dos quadrados dos resíduos, ou seja, a soma dos quadrados das diferenças entre os valores observados e os valores estimados pelo modelo.

A estrutura geral do modelo de regressão linear em forma matricial é dada pela equação:

\begin{equation}
\mathbf{Y}_{[n \times 1]} = \mathbf{X'}_{[n \times p]}\boldsymbol{\beta}_{[p \times 1]} + \boldsymbol{\varepsilon}_{[n \times 1]}
\end{equation} em que $\mathbf{Y}$ é o vetor de variáveis dependentes, $\mathbf{X}$ é a matriz de variáveis independentes, $\boldsymbol{\beta}$ é o vetor de parâmetros e $\boldsymbol{\varepsilon}$ é o vetor de erros aleatórios.

Os parâmetros são estimados por meio da equação:

\begin{equation}
\boldsymbol{b} = (\mathbf{X'X})^{-1}\mathbf{X'Y}
\end{equation} em que $\boldsymbol{b}$ é o vetor de parâmetros estimados.

Para fazer inferências e os testes de hipóteses, é necessário estimar a matriz de variância e covariância dos parâmetros. Isso pode ser feito a partir da estimação dos parâmetros e por meio da equação:

\begin{equation}
\mathbf{Var}(\boldsymbol{b}) = \sigma^2(\mathbf{X'X})^{-1}
\end{equation} em que $\mathbf{V}(\boldsymbol{b})$ é a matriz de variância e covariância dos parâmetros e $\sigma^2$ é a variância dos erros aleatórios. 

Por fim, como os erros aleatórios não são observados, é preciso estimar a variância dos erros aleatórios. Isso pode ser feito por meio da equação:

\begin{equation}
\mathbf{Var}(\boldsymbol{b}) = \frac{SQ_{erros}}{n - p} \sigma^2(\mathbf{X'X})^{-1} = MSE(\mathbf{X'X})^{-1}
\end{equation} em que $\hat{\sigma}^2$ é a estimativa da variância dos erros aleatórios, $SQ_{erros}$ é a soma dos quadrados dos erros, $n - p$ é o número de graus de liberdade do modelo, e $MSE$ é o erro médio quadrático.




### Testes de ausência de regressão e de significância dos parâmetros


A primeira análise de um modelo consiste em testar a hipótese nula de ausência de regressão. Isso é feito por meio de um teste F, cuja estatística é dada por:

\begin{equation}
F = \frac{SQ_{reg}}{p - 1} \div \frac{SQ_{erros}}{n - p}
\end{equation} em que $SQ_{reg}$ é a soma dos quadrados da regressão, $p - 1$ é o número de graus de liberdade da regressão, $SQ_{erros}$ é a soma dos quadrados dos erros e $n - p$ é o número de graus de liberdade dos erros. 

Se os erros tiverem distribuição normal, a estatística $F$ segue uma distribuição $F$ com $p - 1$ e $n - p$ graus de liberdade. 

Já os a validade estatística dos parâmetros pode ser testada por meio de um teste $t-student$, cuja estatística é dada por:

\begin{equation}
t^* = \frac{b_j - \beta_j}{\sqrt{\mathbf{s^2}(b_j)}}
\end{equation} em que $b_j$ é o parâmetro estimado, $\beta_j$ é o parâmetro teórico, $\mathbf{s^2}(b_j)$ é a variância do parâmetro estimado e $t*$ é a estatística do teste que tem distribuição $t-student_{(n-p)}$. Normalmente, testa-se a hipótese nula de que o parâmetro é igual a zero, ou seja, $H_0: \beta_j = 0$. 


## Validação do modelo 

Por fim, após todos os procedimentos acima indicados, deve-se testar a validade do modelo e sua capacidade de generalização. Para isso, é preciso testar o modelo em uma amostra diferente daquela usada para a estimação dos parâmetros.

No caso em análise, será calculado um modelo com uma amostra de tamanho 60. 

Em primeiro lugar, será calculado um novo modelo com os demais dados do problema, que fazem parte do conjunto de validação. Os parâmetros deste modelo são comparados aos parâmetros do modelo do conjunto de treinamento. Caso haja estabilidade dos parâmetros, pode-se dizer que o modelo é consistente com toda a população.

Em seguida, será calculado o erro médio quadrático (MSE) do modelo originalmente treinado no conjunto de validação. O MSE é calculado por meio da equação:

\begin{equation}
MSE = \frac{SQ_{erros}}{n - p}
\end{equation} em que $SQ_{erros}$ é a soma dos quadrados dos erros, $n - p$ é o número de graus de liberdade do modelo.

Espera-se, com isso, que o modelo tenha um MSE próximo ao MSE do modelo de treinamento. Teste semelhante pode ser feito com o coeficiente de determinação ($R^{2}$) e o $R^{2}$ ajustado.

Por fim, caso o modelo escolhido se comporte bem nas duas análises indicadas acima, estima-se o mesmo modelo, desta vez utilizando o conjunto de dados completo. Por se tratar de um tamanho de amostra maior, espera-se que a precisão do modelo seja mais elevada. Com isso, tem-se um modelo final, que pode ser usado para a explicar a relação entre as variáveis e para a previsão de valores da variável dependente a partir de valores conhecidos das variáveis independentes.
